{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bolorunduro/BACPI/blob/master/Copy_of_Fake_News_Detection_GloVe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJbYXou6chZf",
        "outputId": "e504a2d2-a319-4970-c58f-b762f03f3e9f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eO_TCYn-vNT"
      },
      "source": [
        "## **Fake News Classification**\n",
        "---\n",
        "\n",
        "The evolution of the information and communication technologies has dramatically increased the number of people with access to the Internet, which has changed the way the information is consumed.\n",
        "\n",
        "As a consequence of the above, fake news have become one of the major concerns because its potential to destabilize governments, which makes them a potential danger to modern society.\n",
        "\n",
        "An example of this can be found in the US. electoral campaign, where the term \"fake news\" gained great notoriety due to the influence of the hoaxes in the final result of these.\n",
        "\n",
        "\n",
        "<p align=\"center\"><img src=\"https://ichef.bbci.co.uk/news/904/cpsprodpb/15B3B/production/_99919888_fakenews4.gif\" width=\"80%\"/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A807rOtumuf1"
      },
      "source": [
        "Easiest way to download kaggle data in Google Colab\n",
        "Posted in Kaggle.\n",
        "\n",
        "Please follow the steps below to download and use kaggle data within Google Colab:\n",
        "\n",
        "1. First join the competition\n",
        "2. Go to your account, Scroll to API section and Click Expire API Token to remove previous tokens\n",
        "\n",
        "3. Click on Create New API Token - It will download kaggle.json file on your machine.\n",
        "\n",
        "4. Go to your Google Colab project file and run the following commands:\n",
        "  ```\n",
        "  1) ! pip install -q kaggle\n",
        "\n",
        "  2) from google.colab import files\n",
        "\n",
        "  files.upload()\n",
        "\n",
        "  Choose the kaggle.json file that you downloaded\n",
        "  3) ! mkdir ~/.kaggle\n",
        "\n",
        "  ! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "  Make directory named kaggle and copy kaggle.json file there.\n",
        "  4) ! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "  Change the permissions of the file.\n",
        "  5) ! kaggle datasets list\n",
        "  ```\n",
        "- That's all ! You can check if everything's okay by running this command.\n",
        "\n",
        "  Download Data\n",
        "  ```\n",
        "  ! kaggle competitions download -c 'name-of-competition'\n",
        "  ```\n",
        "  Use unzip command to unzip the data:\n",
        "\n",
        "  For example,\n",
        "\n",
        "  Create a directory named train,\n",
        "  ```\n",
        "  ! mkdir train\n",
        "  ```\n",
        "  unzip train data there,\n",
        "  ```\n",
        "  ! unzip train.zip -d train\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSAf1QYTFezp"
      },
      "source": [
        "## **Two ways to import file**\n",
        "\n",
        "1.  From local machine to Colab\n",
        "2.  Import file from Drive to Colab\n",
        "\n",
        "\n",
        "We go for 2nd method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDOnujqi-4Wm"
      },
      "source": [
        "\n",
        "## **Import Json file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "79726gYh-9wg",
        "outputId": "1e50e738-f292-4ee7-b95c-439699cabaa9"
      },
      "source": [
        "# Import kaggle.json from local computer\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "       print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-95990fc4-d564-4f81-8720-f7cf4dcf82b6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-95990fc4-d564-4f81-8720-f7cf4dcf82b6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYdcG6XbAho9"
      },
      "source": [
        "## **Mount Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgxhplOz_uFd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TY7wCoj62UP"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks/Project/Kaggle - Fake-and-Real-News-Dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CejO7xy4j8jm"
      },
      "source": [
        "### **`Extract Kaggle.json file`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uC3myrl7dGS"
      },
      "source": [
        "print(\"[INFO] Extracting kaggle.json file from Drive .....\")\n",
        "!unzip -q \"/content/drive/My Drive/Colab Notebooks/Project/Kaggle-Fake-News-Dataset/kaggle.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpFtYyohmZSM"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHDsB1xwqTN-"
      },
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXwtM_oTqTB1"
      },
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpugMYD3rHDA"
      },
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAlwoaFLqFvv"
      },
      "source": [
        "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ97bWtJLX4o"
      },
      "source": [
        "!unzip fake-and-real-news-dataset.zip -d fake-and-real-news-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vSavC1i2kM1"
      },
      "source": [
        "## **Objective of the project**\n",
        "\n",
        "The goal of this machine learning project is to predict whether a news is fake or not based on the combination\n",
        "**text, title and author features**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DTjBuJN8u0g"
      },
      "source": [
        "Basic of Machine Learning\n",
        "\n",
        "```\n",
        "Steps :\n",
        "                1) Data import  \n",
        "                2) Data preprocessing                \n",
        "                3) Data cleaning\n",
        "                4) Data visualization\n",
        "                5) Split the Data into training/test sets\n",
        "                4) Create a model\n",
        "                5) Train the model\n",
        "                6) Make prediction\n",
        "                7) Evaluate and Improve\n",
        "                8) Performance measure / Metric\n",
        "```                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhb3uwiw-Zb3"
      },
      "source": [
        "## **Contents**\n",
        "\n",
        "1. Dataset Information\n",
        "2. Importing important libraries\n",
        "3. Reading dataset\n",
        "4. Data Pre-Processing\n",
        "5. Building model\n",
        "6. Spiliting and Training\n",
        "7. Submission file\n",
        "8. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLcAv0S06sIJ"
      },
      "source": [
        "## **`Load data`**\n",
        "\n",
        "Download the dataset by visiting the [Kaggle - Fake news dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset) and click the “Download All” button.\n",
        "\n",
        "<hr/>\n",
        "\n",
        "### **`Folder structure`**\n",
        "\n",
        "<hr/>\n",
        "\n",
        "📁 fake-and-real-news-dataset<br/>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  📄  Fake.csv <br/>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  📄  True.csv <br/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_32T2AyBrNK"
      },
      "source": [
        "\n",
        "## **`Installing required package`**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wra4JD1DBqVR"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzz1c1P_wohD"
      },
      "source": [
        "!pip install textblob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP5dPHQYDBFi"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlFmtW2E_db4"
      },
      "source": [
        "## **Import package**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJLP_UOuwmS2"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_tDd9BN9whg"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRcHiXpV_gZz"
      },
      "source": [
        "from tensorflow.keras.models import Sequential                        # Sequential model\n",
        "from tensorflow.keras.preprocessing.text import one_hot               # One hot vector\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer             # Tokenizer keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences     # Pad Sequence\n",
        "from tensorflow.keras.layers import Embedding                         # Embeddings\n",
        "from tensorflow.keras.layers import LSTM                              # LSTM\n",
        "from tensorflow.keras.layers import Dropout                           # Dropout\n",
        "from tensorflow.keras.layers import Dense                             # Dense layer\n",
        "from tensorflow.keras.layers import Bidirectional                     # BiDirectional\n",
        "from sklearn.feature_extraction.text import CountVectorizer           # Bag of words\n",
        "from sklearn.model_selection import train_test_split                  # Train\n",
        "\n",
        "from tensorflow.keras.callbacks import (EarlyStopping,                # Checkpoints\n",
        "                                      ReduceLROnPlateau,\n",
        "                                      ModelCheckpoint,\n",
        "                                      TensorBoard)\n",
        "\n",
        "from sklearn.metrics import ( classification_report,                  # Metrics for calculation\n",
        "                            confusion_matrix,\n",
        "                            accuracy_score )\n",
        "\n",
        "from tensorflow.keras.utils import plot_model       # Plot model architecture\n",
        "import matplotlib.pyplot as plt                     # library for visualization\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import cufflinks                                    # We need cufflinks to link plotly to pandas\n",
        "cufflinks.go_offline()\n",
        "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
        "\n",
        "\n",
        "from plotly.offline import init_notebook_mode, iplot    # IPLOT Library\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "\n",
        "import ipywidgets as widgets                        # Display Button, HTML pages\n",
        "from ipywidgets import Layout\n",
        "from IPython.display import display\n",
        "from pprint import pprint                           # Beautiful printing\n",
        "\n",
        "import pickle                                       # Use pickle to save tokenizer\n",
        "import random                                       # pseudo-random number generator\n",
        "import numpy as np                                  # linear algebra\n",
        "import pandas as pd                                 # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm, tqdm_notebook                # Description and additional stats, Nested progress bars\n",
        "from wordcloud import WordCloud                     # Wordcloud visualization (Most frequent words)\n",
        "\n",
        "import nltk                                         # Python library for NLP\n",
        "import re                                           # library for regular expression operations\n",
        "import string                                       # for string operations\n",
        "from nltk.corpus import stopwords                   # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer                 # module for stemming\n",
        "from nltk.stem import WordNetLemmatizer             # module for Lemmatizer\n",
        "from nltk.tokenize import TweetTokenizer            # module for tokenizing strings\n",
        "import contractions                                 # contraction libary\n",
        "import unicodedata                                  # charachter encoding library\n",
        "from textblob import TextBlob, Word                 # textBlob library\n",
        "from collections import defaultdict,Counter         # collection library\n",
        "from nltk.corpus import stopwords                   # stopwords\n",
        "import spacy                                        # NLP library for features NER, POS tagging, dependency parsing\n",
        "from spacy import displacy                          # Render a dependency parse tree or named entity visualization\n",
        "from nltk.util import ngrams                        # Display N Grams\n",
        "\n",
        "nltk.download('stopwords')                          # download the stopwords from NLTK\n",
        "nltk.download('wordnet')                            # Wordnet\n",
        "nltk.download('averaged_perceptron_tagger')         # POS Tagger\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Nos8k-DJU3"
      },
      "source": [
        "import gc\n",
        "import warnings                             # For removing \"Warnings\"\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gf62t_yCKOi"
      },
      "source": [
        "## **Color text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHcJrc1dCJ36"
      },
      "source": [
        "# just for display\n",
        "LIGHTRED ='\\033[91m'\n",
        "LIGHTGREEN ='\\033[92m'\n",
        "LIGHTBLUE ='\\033[94m'\n",
        "BLACK = '\\033[30m'\n",
        "RED = '\\033[31m'\n",
        "GREEN = '\\033[32m'\n",
        "YELLOW = '\\033[33m'\n",
        "BLUE = '\\033[34m'\n",
        "MAGENTA = '\\033[35m'\n",
        "CYAN = '\\033[36m'\n",
        "WHITE = '\\033[37m'\n",
        "BOLD ='\\033[01m'\n",
        "LIGHTCYAN ='\\033[96m'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNklcZ769-f4"
      },
      "source": [
        "## **Dataset information**\n",
        "\n",
        "```\n",
        "fake.csv: A full training dataset with the following attributes:\n",
        "\n",
        "title: the title of a news article\n",
        "text: the text of the article; could be incomplete\n",
        "subject: subject of the news article\n",
        "date : Date of article posted\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvaCTEDntc_X"
      },
      "source": [
        "# Read Dataset\n",
        "fake_news = pd.read_csv('fake-and-real-news-dataset/Fake.csv')\n",
        "true_news = pd.read_csv('fake-and-real-news-dataset/True.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QTYIqVnGg6l"
      },
      "source": [
        "# check top 5 row of the dataset (Default is 5) - Fake News\n",
        "fake_news.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzTwMJYBGlz1"
      },
      "source": [
        "# check bottom 5 row of the dataset (Default is 5) - Fake News\n",
        "fake_news.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edL0FhwAnzOG"
      },
      "source": [
        "# check top 5 row of the dataset (Default is 5) - true news\n",
        "true_news.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L61H96KHn3Vm"
      },
      "source": [
        "# check bottom 5 row of the dataset (Default is 5) - true News\n",
        "true_news.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae95WhbWHrne"
      },
      "source": [
        "# Printing the dimensions of data\n",
        "'''\n",
        "Output : (no. of rows, no. of columns)\n",
        "'''\n",
        "\n",
        "print(\"Fake news shape : {}\".format(fake_news.shape))\n",
        "print(\"True news shape : {}\".format(true_news.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4euLMsuIGEc"
      },
      "source": [
        "# Printing the columns of the data\n",
        "fake_news.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iPGdGaiqFod"
      },
      "source": [
        "# Printing the columns of the data\n",
        "true_news.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHvxceFDIIsw"
      },
      "source": [
        "# information retrive from the dataset\n",
        "fake_news.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv-n32GiqIxM"
      },
      "source": [
        "# information retrive from the dataset\n",
        "true_news.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXg1_6SmIMzn"
      },
      "source": [
        "# Discription of the dataset\n",
        "fake_news.describe(include='O')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSDE_TiNG5md"
      },
      "source": [
        "We can see that  **`title`**  and **`text`** column has many duplicates data, because there are total **`23841`** numbers of row, but unique are  **`17903`** for title and  **`17455`** for text columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a20UUcgsRIc"
      },
      "source": [
        "# Discription of the dataset\n",
        "true_news.describe(include='O')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EL9bYJbtXIx"
      },
      "source": [
        "Similarly there are total **`21417`** numbers of row, but unique are  **`20826`** for title and  **`21192`** for text columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sstzRsz9_l2G"
      },
      "source": [
        "# Checking for null values present in fake news dataset\n",
        "fake_news.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70x4-r4c_qNd"
      },
      "source": [
        "# Checking for null values present in true news dataset\n",
        "true_news.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9py9NN_mCk4b"
      },
      "source": [
        "## **Twitter - User mention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXropBwZCG-w"
      },
      "source": [
        "fake_news['text'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ltq97WiCVv0"
      },
      "source": [
        "A first characteristic that I found curious was the fact that fake news has many mentions of quotes from twitter users, to see the differentiation between true and false news, I raised the amount of quotes in false and true news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3mWCw_BCQMu"
      },
      "source": [
        "def count_twitters_user(df):\n",
        "    twitter_username_re = re.compile(r'@([A-Za-z0-9_]+)')\n",
        "    count = 0\n",
        "    list_ = []\n",
        "    for text in df['text']:\n",
        "        count += len(re.findall(twitter_username_re, text))\n",
        "    return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-g_0lIQaKS_"
      },
      "source": [
        "# Twitter user mention - in fake news\n",
        "twitter_users_fake_count = count_twitters_user(fake_news)\n",
        "\n",
        "# Twitter user mention - in true news\n",
        "twitter_users_true_count = count_twitters_user(true_news)\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Bar(x=['Fake', 'True'],\n",
        "    y=[twitter_users_fake_count, twitter_users_true_count],\n",
        "    name='Twitter user name Pattern',\n",
        "    marker_color='indianred')\n",
        ")\n",
        "fig.update_layout({\n",
        "        'plot_bgcolor': 'rgba(255, 255, 255, 1)',\n",
        "        'paper_bgcolor': 'rgba(255, 255, 255, 1)',\n",
        "        'title': 'unique hashtags mentions in twitters',\n",
        "})\n",
        "#fig = px.bar(y=[twitter_users_fake_count, twitter_users_true_count], x=['Fake', 'True'], title='Twitter user name Pattern')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv2Fo0_sDlAb"
      },
      "source": [
        "- Regarding the real news, fake news has a much greater presence of @ twitter, the sources from which the fake news was collected must be those types of forums where the news mix with twitters.\n",
        "So this may be one of the first features that can reinforce the fact that the dataset is biased.\n",
        "\n",
        "- But perhaps still just the fact that mentions with @ of users would not be enough to affirm that the data is not suitable for identification of fake news, since any preprocessing that removes @ would be able to decrease the bias. So I went in search of another aspect like the text size, to see if there is a very discrepant difference between the texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKt6STPaKcsU"
      },
      "source": [
        "## **Data cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrtzgIrHuKAh"
      },
      "source": [
        "Real news seems to have source of publication which is not present in fake news set\n",
        "\n",
        "### **Looking at the data**\n",
        "\n",
        "- most of text contains reuters information such as \"WASHINGTON (Reuters)\".\n",
        "- Some text are tweets from Twitter\n",
        "- Few text do not contain any publication info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taGxT5HWyCrK"
      },
      "source": [
        "---\n",
        "Removing Reuters or Twitter Tweet information from the text\n",
        "\n",
        "- Text can be splitted only once at \" - \" which is always present after mentioning source of publication, this gives us publication part and text part\n",
        "- If we do not get text part, this means publication details was't given for that record\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKcmUeKKeBHj"
      },
      "source": [
        "# First Creating list of index that do not have publication part\n",
        "unknown_publishers = []\n",
        "for index,row in enumerate(true_news.text.values):\n",
        "    try:\n",
        "        record = row.split(\" -\", maxsplit=1)\n",
        "        #if no text part is present, following will give error\n",
        "        record[1]\n",
        "        # if len of piblication part is greater than 260\n",
        "        #following will give error, ensuring no text having \"-\" in between is counted\n",
        "        '''\n",
        "        assert <condition>,<error message>\n",
        "        #The assert condition must always be True, else it will stop execution and return the error message in the second argument\n",
        "        assert 1==2 , \"Not True\" #returns 'Not True' as Assertion Error.\n",
        "        '''\n",
        "        assert(len(record[0]) < 260)\n",
        "    except:\n",
        "        unknown_publishers.append(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m_VBG91ijV7"
      },
      "source": [
        "true_news.iloc[unknown_publishers].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vp5L-L5ipzK"
      },
      "source": [
        "# the text does not contain publication part\n",
        "true_news.text[3488]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAPW5M91i9nW"
      },
      "source": [
        "# Seperating Publication info, from actual text\n",
        "publisher = []\n",
        "tmp_text = []\n",
        "for index,row in enumerate(true_news.text.values):\n",
        "    if index in unknown_publishers:\n",
        "        # Add unknown of publisher not mentioned\n",
        "        tmp_text.append(row)\n",
        "\n",
        "        publisher.append(\"Unknown\")\n",
        "        continue\n",
        "    record = row.split(\" -\", maxsplit=1)\n",
        "    publisher.append(record[0])\n",
        "    tmp_text.append(record[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4o8yti4jH5n"
      },
      "source": [
        "# Replace existing text column with new text\n",
        "# add seperate column for publication info\n",
        "true_news[\"publisher\"] = publisher\n",
        "true_news[\"text\"] = tmp_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30gNRA0djO7H"
      },
      "source": [
        "# Display the top 5 rows\n",
        "true_news.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtBg1aBOFXkx"
      },
      "source": [
        "# Count of unknown publisher\n",
        "len(true_news[true_news['publisher']=='Unknown'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z3k3VvsjzGw"
      },
      "source": [
        "# Those doesn't contain the publication\n",
        "len(true_news[true_news['publisher'] == None])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD8CYfDQGEVH"
      },
      "source": [
        "true_news['publisher'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYov6RHZGRTT"
      },
      "source": [
        "len(true_news['publisher'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjWG98Xvx38e"
      },
      "source": [
        "len(true_news[true_news['text'] == ' '])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOZTAI5lxk8S"
      },
      "source": [
        "# Checking for rows with empty text\n",
        "empty_true_index = [index for index,text in enumerate(true_news.text.values) if str(text).strip() == '']\n",
        "# seems only one :)\n",
        "print(f\"Empty index : {empty_true_index}\")\n",
        "print(f\"No of empty rows: {len(empty_true_index)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpQ4FIJo95VZ"
      },
      "source": [
        "# Verifying the result\n",
        "true_news.iloc[8970]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCTGizHc84ct"
      },
      "source": [
        "# Checking for the same in fake news\n",
        "empty_fake_index = [index for index,text in enumerate(fake_news.text.values) if str(text).strip() == '']\n",
        "print(f\"No of empty rows: {len(empty_fake_index)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1sTNOUbJiQY"
      },
      "source": [
        "fake_news.iloc[empty_fake_index].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16BQycvhzCTY"
      },
      "source": [
        "We will remove this row in below few section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c-3SHtfywps"
      },
      "source": [
        "### **Labeling true and false news**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iljjvt8jyrBB"
      },
      "source": [
        "true_news['label'] = 1\n",
        "fake_news['label'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYf3ernly_l9"
      },
      "source": [
        "true_news.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUeKew5gzY8k"
      },
      "source": [
        "fake_news.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxb137b0L_m8"
      },
      "source": [
        "# Drop the column 'publisher'\n",
        "true_news = true_news.drop(columns=['publisher'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j2QCEv2ME-0"
      },
      "source": [
        "# Printing the dimensions of data\n",
        "'''\n",
        "Output : (no. of rows, no. of columns)\n",
        "'''\n",
        "print(\"Fake news shape : {}\".format(fake_news.shape))\n",
        "print(\"True news shape : {}\".format(true_news.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tILTvYYSzd6v"
      },
      "source": [
        "### **Merging true and fake news dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYaldxkxziNU"
      },
      "source": [
        "# Merging the 2 datasets row wise : axis = 0 (default)\n",
        "\n",
        "data = pd.concat([true_news,fake_news],ignore_index=True, sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlLRLQic0vvT"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lWs8sHW0uHS"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOS1SvaFKdhB"
      },
      "source": [
        "### **Remove duplicates**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvMaznj_yMIN"
      },
      "source": [
        "# Return duplicate rows based on title and text\n",
        "data[data.duplicated(subset=['title','text'], keep='first')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWgycKu0NBVT"
      },
      "source": [
        "data.title[445]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RcGrFO9NL2o"
      },
      "source": [
        "data[data['title']==\"Senate tax bill stalls on deficit-focused 'trigger'\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zBADYQOKCjp"
      },
      "source": [
        "For Droping rows we are only going to consider two primary features\n",
        "1. title\n",
        "2. text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAg9lmx910MB"
      },
      "source": [
        "# Droping rows based on 'title' and 'text'\n",
        "# When inplace=True is passed, then data is renamed in place (it returns nothing) , if set to 'False', then it return modified data\n",
        "# keep = If ‘first’, This considers first value as unique and rest of the same values as duplicate.\n",
        "\n",
        "data = data.drop_duplicates(subset={\"title\",\"text\"}, keep='first', inplace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R17TC-yg2D0h"
      },
      "source": [
        "# We can see the difference => 44898 - 39104 = 5794\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77EMoQ2dK3Dj"
      },
      "source": [
        "### **Removing null values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPbV1on6Jlnd"
      },
      "source": [
        "'''\n",
        "Check whether their is any null value in column\n",
        "'''\n",
        "data.isna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow8VDWXbKrgO"
      },
      "source": [
        "# We count the number of missing values for each feature (column wise)\n",
        "# below sum shows there are number of null values in the dataset.\n",
        "\n",
        "# Now we verify whether the null rows have been deleted or not\n",
        "\n",
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6R4GtQ0RsHc"
      },
      "source": [
        "# By using dropna method,\n",
        "# we drop the row containing null value\n",
        "\n",
        "'''\n",
        " how :\n",
        "       ‘any’ : If any NA values are present, drop that row or column.\n",
        "        ‘all’ : If all values are NA, drop that row or column.\n",
        "\n",
        " When inplace=True is passed, the data is renamed in place (it returns nothing)\n",
        "'''\n",
        "\n",
        "# data.dropna(columns = 'text', how='any',inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYbtdKj5SBFF"
      },
      "source": [
        "### **Merge text and title column**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Q2qL8aSII0"
      },
      "source": [
        "data['combine_text'] = data['title'] + \" \" + data['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCKeMHZBSVkT"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q45oSEEJMHLO"
      },
      "source": [
        "### **Removing rows if word_count < 50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp9pL3pQLaH_"
      },
      "source": [
        "# Word count of text\n",
        "data['length'] = [len(s.split()) for s in data['text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHGLhEnkSkI2"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbSVXFc-Zu9T"
      },
      "source": [
        "### **PLOT - Box Plot and Whisker plot**\n",
        "\n",
        "Boxplots are a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”).\n",
        "\n",
        "<p align=\"center\"><img src=\"https://miro.medium.com/max/18000/1*2c21SkzJMf3frPXPAR_gZA.png\" width=\"80%\"/></p>\n",
        "\n",
        "\n",
        "The image above is a boxplot. A boxplot is a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”). It can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.\n",
        "\n",
        "\n",
        "**Reference**\n",
        "\n",
        "- [TowerdsDataScience - Understanding Box Plot](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_CeGOq2XgOF"
      },
      "source": [
        "trace1 = go.Box(\n",
        "    y=data['length'][data['label']==0],\n",
        "    name = 'Fake Text',\n",
        "    marker = dict(\n",
        "        color = 'green',\n",
        "    )\n",
        ")\n",
        "\n",
        "trace2 = go.Box(\n",
        "    y=data['length'][data['label']==1],\n",
        "    name = 'Real Text',\n",
        "    marker = dict(\n",
        "        color = 'red',\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "res = [trace1, trace2]\n",
        "layout = go.Layout(\n",
        "    title = \"Length of the text\"\n",
        ")\n",
        "\n",
        "fig = go.Figure(res,layout=layout)\n",
        "iplot(fig, filename = \"Length of the text of different polarities\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDlLibs3_8T5"
      },
      "source": [
        "Fake news in general has a lot more tokens than real ones which is kind of weird 🤔, assuming real news has a tendency to bring details about events to inform the reader, however as noted the fact that fakes news is a mix of twitters and news this may justify the fact that they have more words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH3uaiTYUNAs"
      },
      "source": [
        "# Number of datapoints which text length is greater > 1000\n",
        "len(data[data['length']>=1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRQzBoy4UXE6"
      },
      "source": [
        "# Number of datapoints which text length is greater > 1000 and which are \"fake\"\n",
        "len(data[(data['length']>=1000) & (data['label']==0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwqoJcBcUn_3"
      },
      "source": [
        "# Number of datapoints which text length is greater > 1000 and which are \"real\"\n",
        "len(data[(data['length']>=1000) & (data['label']==1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvAIUQzdMBak"
      },
      "source": [
        "# Display the data which has word count length < 50\n",
        "data[data['length']<50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e629WTSGO-Wk"
      },
      "source": [
        "# Total rows whose length is < 50\n",
        "len(data[data['length'] < 50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eduPNDsAOyon"
      },
      "source": [
        "There are 1749 outliers in this dataset. Outliers can be removed. It is a good practice to check the outliers before removing them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy68KHWsTT1i"
      },
      "source": [
        "data['text'][428]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnkVgEj6VR_2"
      },
      "source": [
        "Here we can see that the text whose length is 50 contains the words which are very significant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtssRAedTSn5"
      },
      "source": [
        "# Display the data which has word count length < 50\n",
        "data[data['length']<30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kne9kbgQWVZ_"
      },
      "source": [
        "data['combine_text'][502]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JZ9woZ3cgb1"
      },
      "source": [
        "data['text'][502]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdqw0GEfPV_Q"
      },
      "source": [
        "# dropping the outliers\n",
        "data = data.drop(data['text'][data['length'] < 30].index, axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLOLmlrFX9ju"
      },
      "source": [
        "# Rows has been reduced from 39104 to 37998\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbPlujoQPsCt"
      },
      "source": [
        "Mostly empty texts. They can be removed since they will surely guide the neural network in the wrong way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVFWZCuld_AL"
      },
      "source": [
        "#Import the english stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "print('Stop words\\n')\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nlength of stopwords list\\n')\n",
        "print(len(stopwords_english))\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q60ITU9yzzUZ"
      },
      "source": [
        "We can see that the stop words list above contains some words that could be important in some contexts. These could be words like **`'not'`**, **`don't`**, **`weren't`**, **`between`**, **`because`**, **`won`**, **`against`**.\n",
        "\n",
        "> **Before stopwords processing** : I don't like this food ( negative sentiment)\n",
        "\n",
        "> **After stopwords processing** : I like this food  ( positive sentiment)\n",
        "\n",
        "Notice negative sentiment, get's convert into positive sentiment.\n",
        "\n",
        "Note all contraction words also gets removed, during removal of stopwords.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.pinimg.com/originals/48/f1/2d/48f12d71022f633a2d2af3ea80d45f10.png\" width=\"80%\"/></p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z105kTYtqpUB"
      },
      "source": [
        "# The word which we don't want to remove\n",
        "operators = set(('not', 'against', 'because', \\\n",
        "                  'until', 'against', 'between', 'during', 'into', 'before', 'after', 'no', 'nor','won'))\n",
        "\n",
        "# Remove this word from stopwords\n",
        "stopwords_english = set(stopwords_english) -  operators\n",
        "\n",
        "print(f'{list(stopwords_english)}\\n')\n",
        "\n",
        "# see the length is gets reduce i.e from 179 to 165\n",
        "print(len(stopwords_english))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0bbq0HSOrsP"
      },
      "source": [
        "### **Plot top stopwords**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWkd1bhhCqT5"
      },
      "source": [
        "# Before removing checking occurances of stopwords in headline\n",
        "\n",
        "def plot_top_stopwords_barchart(text,stopwords):\n",
        "\n",
        "    # stop=set(stopwords.words('english'))\n",
        "\n",
        "    new= text.str.split()\n",
        "    new=new.values.tolist()\n",
        "\n",
        "    corpus=[word for i in new for word in i]\n",
        "\n",
        "    dic=defaultdict(int)\n",
        "    for word in corpus:\n",
        "        if word in stopwords:\n",
        "            dic[word]+=1\n",
        "\n",
        "    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n",
        "    x,y=zip(*top)\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "\n",
        "    plt.title('frequency plot of top stopwords', fontsize=20)\n",
        "    plt.xlabel('Stopwords')\n",
        "    plt.ylabel('Frequency\\'s of stopwords')\n",
        "    plt.bar(x,y)\n",
        "\n",
        "plot_top_stopwords_barchart(data['combine_text'],stopwords_english)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsk_gwnlOlqr"
      },
      "source": [
        "### **Process test function**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tkCif-JZlhl"
      },
      "source": [
        " def process_text(content):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        content: a string containing a text\n",
        "    Output:\n",
        "        content_clean: a list of words containing the processed text\n",
        "    \"\"\"\n",
        "    # Removing Accented Characters (Words such as résumé, café, prótest, divorcé, coördinate, exposé, latté etc.)\n",
        "    content = unicodedata.normalize('NFKD', content).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "    # Decontraction : aren't -> are not\n",
        "    content = contractions.fix(content)\n",
        "\n",
        "    # Remove HTML Element : <br/>, <hr/>, <p/>\n",
        "    content = BeautifulSoup(content, 'lxml').get_text()\n",
        "\n",
        "    # remove stock market tickers like $GE\n",
        "    content = re.sub(r'\\$\\w*', '', content)\n",
        "\n",
        "    # remove old style retweet text \"RT\"\n",
        "    content = re.sub(r'^RT[\\s]+', '', content)\n",
        "\n",
        "    # remove hyperlinks\n",
        "    content = re.sub(r'https?:\\S+','',content)\n",
        "\n",
        "    # to remove other url links (like tim@gmail.com, abcnews.com etc)\n",
        "    content = re.sub(r'[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', content, flags=re.MULTILINE)\n",
        "\n",
        "    # Replaces #hashtag with hashtag\n",
        "    # content = re.sub(r'#(\\S+)', r'\\1', content)\n",
        "\n",
        "    # Replace @user with @user, or @@user with @user\n",
        "    content = re.sub(r'[@]*(\\S+)', r'\\1', content)\n",
        "\n",
        "    # Replace digit or digit before string (like 6pm) or string before digit ( like Hello123) with empty string\n",
        "    content = re.sub('\\w*\\d\\w*','', content)\n",
        "\n",
        "    # Multiline comment\n",
        "    content = re.sub(r'/\\*[^*]*(?:\\*(?!/)[^*]*)*\\*/', '', content , flags=re.S)\n",
        "\n",
        "    # funnnnny --> funny\n",
        "    content = re.sub(r'(.)\\1+', r'\\1\\1', content)\n",
        "\n",
        "    # Replace multiple spaces with a single space or Removing extra whitespaces and tabs\n",
        "    # content = re.sub(r'\\s+', ' ', content)\n",
        "    content = re.sub(r'^\\s*|\\s\\s*', ' ', content).strip()\n",
        "\n",
        "    # remove line break '\\n'\n",
        "    content = re.sub(r\"(?<=[a-z])\\r?\\n\",\" \", content)\n",
        "\n",
        "    # tokenize content\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    content_tokens = tokenizer.tokenize(content)\n",
        "\n",
        "    content_clean = []\n",
        "    for word in content_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation): # remove punctuation\n",
        "            content_clean.append(word)\n",
        "\n",
        "    # Lammetizer\n",
        "    sent = TextBlob(\" \".join(content_clean))\n",
        "    tag_dict = {\"J\": 'a',\n",
        "                \"N\": 'n',\n",
        "                \"V\": 'v',\n",
        "                \"R\": 'r'}\n",
        "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]\n",
        "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
        "    return \" \".join(lemmatized_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz131CA5gwXZ"
      },
      "source": [
        " # Testing\n",
        "\n",
        "sentence = '''\n",
        "            bit.ly/wRwe23 U.S. election, P.M Sómě Áccěntěd těxt 6btqo 6c 6csxqaft1 6d 6d02j8y 6dan 6e 6e5nhvt\n",
        "            ABCD23, hello123world -23 55\n",
        "            «Pourquoi je suis candidat à la présidentielle, Hellllllo guys, i am gratefullll to be part of this team\n",
        "            hritik.dj@somaiya.edu \\t     www.google.com Dikran Arakelian    (noreply@blogger.com)\n",
        "            i'm passsionate about learning new things <br/> <hr/> <p> <h/> \\n\\n ,\n",
        "            http://url.com/bla1/blah1/,  @tim, #covid19, #trump .... | coder | hacker\n",
        "            @digitalINDIA @2021 , @covid19 ,,, @@@helloworld2021 ,,,###newyork, ##murdercase, #justice\n",
        "           /* PUT MULTILINE\n",
        "            COMMENT HERE (between\n",
        "            the two delimeters). */\n",
        "          '''\n",
        "\n",
        "print(process_text(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwlsUvWfUcKW"
      },
      "source": [
        "test_sentence = data['text'][0]\n",
        "test_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7WSlWY14MGQ"
      },
      "source": [
        "# Final test\n",
        "for i in range(5):\n",
        "  tester2 = process_text(data['text'][i])\n",
        "  print(tester2, end='\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wczqmCsLOzev"
      },
      "source": [
        "### **Data Duplication**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwfOQm2jFgic"
      },
      "source": [
        "# Replica of original data (so that original data is not modified during preprocessing )\n",
        "data_duplicate = data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mk7CE0zFjr1"
      },
      "source": [
        "data_duplicate.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAtUgaU7LsBG"
      },
      "source": [
        "data_duplicate.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKwb38MaDCi3"
      },
      "source": [
        "After removing NaN values index number are not in proper order so for managing index we have to do index reseting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUso-sPzE2Cz"
      },
      "source": [
        "data_duplicate = data_duplicate.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC_DeZUBLapM"
      },
      "source": [
        "data_duplicate.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baayoxfGLVml"
      },
      "source": [
        "data_duplicate.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owhPQO4LP3ld"
      },
      "source": [
        "### **Cleaning the `Total` Column**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A777QEbCR3Px"
      },
      "source": [
        "%%time\n",
        "data_duplicate['clean_text_data'] = data_duplicate['combine_text'].apply(lambda x : process_text(str(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKyawohL-epv"
      },
      "source": [
        "data_duplicate.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMDrbXYPqV0Y"
      },
      "source": [
        "### **Save CSV File to drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JUerHUp-i-g"
      },
      "source": [
        "data_duplicate.to_csv('/content/drive/My Drive/Colab Notebooks/Project/Kaggle - Fake-and-Real-News-Dataset/Fake_News_Training.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0wyrMHotnpV"
      },
      "source": [
        "### **Load CSV File from Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vtwpQTnt4Ut"
      },
      "source": [
        "data_duplicate = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Project/Kaggle - Fake-and-Real-News-Dataset/Fake_News_Training.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhgdYADVRX_v"
      },
      "source": [
        "## **Data visualization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2EOZ0mgtxCh"
      },
      "source": [
        "### **Frequency plot of top non stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HollsJPXYuf"
      },
      "source": [
        "def plot_top_non_stopwords_barchart(text, stopwords):\n",
        "\n",
        "    new= text.str.split()\n",
        "    new=new.values.tolist()\n",
        "    corpus=[word for i in new for word in i]\n",
        "\n",
        "    counter=Counter(corpus)\n",
        "    most=counter.most_common()\n",
        "    x, y=[], []\n",
        "    for word,count in most[:50]:\n",
        "        if (word not in stopwords):\n",
        "            x.append(word)\n",
        "            y.append(count)\n",
        "    plt.figure(figsize=(10,10))\n",
        "\n",
        "    plt.title('frequency plot of top non stopwords', fontsize=20)\n",
        "    plt.xlabel('Frequency\\'s of non stopwords')\n",
        "    plt.ylabel('Non Stopwords')\n",
        "    sns.barplot(x=y,y=x)\n",
        "\n",
        "plot_top_non_stopwords_barchart(data_duplicate['clean_text_data'],stopwords_english)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K2Eb1T-eeRH"
      },
      "source": [
        "### **Count plot - Labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Taay6aqM4G"
      },
      "source": [
        "data_duplicate['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hGnbdsuRTox"
      },
      "source": [
        "plt.figure(figsize=(11,7))\n",
        "sns.set_theme()\n",
        "sns.countplot(data_duplicate['label'], palette='Set2',edgecolor=sns.color_palette(\"dark\", 3))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d-kfjQwRmW8"
      },
      "source": [
        "# To also see the value of each attribute in terms of number\n",
        "# we modify our code\n",
        "\n",
        "plt.figure(figsize=(11,7))\n",
        "\n",
        "total = float(len(data[\"label\"]) )\n",
        "\n",
        "ax = sns.countplot(data['label'], palette='Set2',edgecolor=sns.color_palette(\"dark\", 3))\n",
        "\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x()+p.get_width()/2.,\n",
        "            height + 100,\n",
        "            '{:1.2f}'.format((height)),\n",
        "            ha=\"center\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y4POqtJ78ia"
      },
      "source": [
        "### **Unique Tokens plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ROdUECD8EBt"
      },
      "source": [
        "def unique_tokens(df):\n",
        "    unique_tokens = set()\n",
        "    for text in tqdm(df):\n",
        "        splited = text.split()\n",
        "        for token in splited:\n",
        "            unique_tokens.add(token)\n",
        "    return unique_tokens\n",
        "\n",
        "unique_tokens_fake = unique_tokens(data_duplicate['text'][data_duplicate['label']==0])\n",
        "unique_tokens_true = unique_tokens(data_duplicate['text'][data_duplicate['label']==1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZblZxDV58amE"
      },
      "source": [
        "# import plotly.express as px\n",
        "fig = go.Figure(px.bar(data_duplicate,y=[len(unique_tokens_fake), len(unique_tokens_true)], x=['Fake', 'True'], title='Unique tokens'))\n",
        "fig.show(renderer=\"colab\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc-Euk8iGeEN"
      },
      "source": [
        "Fake news has a lot more different tokens than the real ones, this was to be expected assuming that inside the fake there are twitters where people use, abbreviations , slang word and language addictions in non-formal writing.\n",
        "\n",
        "I believe it is interesting to observe the occurrence of words that do not exist in the English language to see the difference between true and false news.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnQGGq5NJY34"
      },
      "source": [
        "### **Spell Checking between Fake and Real News Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_mz0S1gIU5k"
      },
      "source": [
        "!pip install pyenchant\n",
        "!apt-get install libenchant1c2a -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e14_zhRHIibN"
      },
      "source": [
        "import enchant\n",
        "\n",
        "def check_if_exist(list_):\n",
        "    d = enchant.DictWithPWL(\"en_US\", \"vocab.txt\")\n",
        "    count = 0\n",
        "    for token in tqdm(list_):\n",
        "        if not d.check(token) and not d.check(token.capitalize()):\n",
        "            count+=1\n",
        "    return count\n",
        "\n",
        "count_fake = check_if_exist(unique_tokens_fake)\n",
        "count_true = check_if_exist(unique_tokens_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCScrPCdI2wf"
      },
      "source": [
        "fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n",
        "\n",
        "fig.append_trace(go.Pie(values=[count_fake, len(unique_tokens_fake)-count_fake],\n",
        "                        labels=['Non exist', 'exist'], hole=.7,\n",
        "                        title='Fake News'), row=1, col=1)\n",
        "\n",
        "fig.append_trace(go.Pie(values=[count_true, len(unique_tokens_true)-count_true],\n",
        "                        labels=['Non exist', 'exist'], hole=.7,\n",
        "                        title='Real News'), row=1, col=2)\n",
        "fig.show(renderer=\"colab\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwGFkk0eJzOd"
      },
      "source": [
        "Not exist - Words are misspelled (not present in english dictionary).\n",
        "\n",
        "Exist - Present in english dictionary.\n",
        "\n",
        "More than 70% of the words in the news fakes were not found in the dictionary used for verification,\n",
        "it is important to make it clear that it is not a perfect dictionary but that it already brings this section that many words are really misspelled.\n",
        "\n",
        "```\n",
        "The misspellings definitely make sense, real news has copy editors that look for spelling mistakes.\n",
        "Fake news definitely does not have the same rigorous review process.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uouVCIKNevWh"
      },
      "source": [
        "### **Bar plot - Subject**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ2V0IyXKgCW"
      },
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "color = sns.color_palette(\"Set2\")\n",
        "ax = sns.countplot(x=\"subject\",  hue='label', data=data_duplicate, palette=color)\n",
        "\n",
        "# ax.set(xticklabels=['fake', 'real'])\n",
        "\n",
        "plt.title(\"Data distribution of fake and real data in terms of subject\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37aH3sL6e0im"
      },
      "source": [
        "### **Pie chart - Authors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjPkyXgNQdXt"
      },
      "source": [
        "plt.figure(figsize=(30,10))\n",
        "plt.title('Percentage of Subject', fontsize=20)\n",
        "data_duplicate['subject'].value_counts().plot(kind='pie', \\\n",
        "                              wedgeprops=dict(width=.7), autopct='%1.0f%%', startangle= -20,\n",
        "                              textprops={'fontsize': 15})\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWmlu7vLNci8"
      },
      "source": [
        "### **Word Count histogram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MIfhPAu14BY"
      },
      "source": [
        "We want to look at the word count for each news and see if there is difference between real and fake news.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qaZz6KmK35m"
      },
      "source": [
        "data_duplicate['word_count'] = [len(s.split()) for s in data_duplicate['clean_text_data']]\n",
        "\n",
        "# Alternative\n",
        "# data_duplicate['word_count'] = data_duplicate['clean_text_data'].apply(lambda x: len(str(x).split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-0C4VhmTl5D"
      },
      "source": [
        "data_duplicate.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjj0pH3MiJ31"
      },
      "source": [
        "data_duplicate['word_count'][data_duplicate['label']==0].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h6SDn0NiUPv"
      },
      "source": [
        "# Density plot of fake news\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Density plot of word count [Fake]', fontsize=20)\n",
        "sns.distplot(data_duplicate['word_count'][data_duplicate['label'] == 0], kde=False, rug=False, \\\n",
        "                            hist_kws={\"linewidth\": 3,\n",
        "                            \"alpha\": 1, \"color\": \"b\"})\n",
        "plt.xlabel('word_count')\n",
        "plt.ylabel('frequency\\'s of word count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWrA4rq5Iq5N"
      },
      "source": [
        "We can also observe that 500 no. of words in news are more frequent then 1000 words.\n",
        "\n",
        "We can see from the above graph that most fake news are within 2000 words, and the distribution of word count is skewed to the right.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cfGFndlRxHX"
      },
      "source": [
        "# Fake news\n",
        "max(data_duplicate['word_count'][data_duplicate['label']==0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CLnqs8WR2L3"
      },
      "source": [
        "There might be a some text in dataset whose word count is much longer than 2000. (Which can be consider as outlier)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4NmqoxJTTDj"
      },
      "source": [
        "len(data_duplicate['word_count'][(data_duplicate['label'] == 0) & (data_duplicate['word_count'] > 1000)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yD_vi_CXlhI"
      },
      "source": [
        "### **Lets understand the clear picture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08GOEyzcXjTJ"
      },
      "source": [
        "# Fake\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Density plot of word count [Fake]', fontsize=20)\n",
        "sns.distplot(data_duplicate['word_count'][(data_duplicate['label'] == 0) & (data_duplicate['word_count'] < 500)], kde=False, rug=False, \\\n",
        "                            hist_kws={\"linewidth\": 3,\n",
        "                            \"alpha\": 1, \"color\": \"b\"})\n",
        "plt.xlabel('word_count')\n",
        "plt.ylabel('frequency\\'s of word count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbNz7wC7fQkt"
      },
      "source": [
        "# REAL\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Density plot of word count [REAL]', fontsize=20)\n",
        "sns.distplot(data_duplicate['word_count'][data_duplicate['label'] == 1], kde=False, rug=False, \\\n",
        "                            hist_kws={\"linewidth\": 3,\n",
        "                            \"alpha\": 1, \"color\": \"b\"})\n",
        "plt.xlabel('word_count')\n",
        "plt.ylabel('frequency\\'s of word count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIGeuTmk_o_T"
      },
      "source": [
        "max(data_duplicate['word_count'][data_duplicate['label']==1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICcl2DjWYJun"
      },
      "source": [
        "len(data_duplicate['word_count'][(data_duplicate['label'] == 1 ) & (data_duplicate['word_count'] > 1000)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6q5_zjTI2Q8"
      },
      "source": [
        "As for the real news, we see some outliers from above figure, making it hard to intepret, so we plot it again below without outlier (news that has more than 14,000 words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjC7QUM0Sb3N"
      },
      "source": [
        "# REAL\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Density plot of word count [REAL]', fontsize=20)\n",
        "sns.distplot(data_duplicate['word_count'][(data_duplicate['label'] == 1) & (data_duplicate['word_count'] < 700)], kde=False, rug=False, \\\n",
        "                            hist_kws={\"linewidth\": 3,\n",
        "                            \"alpha\": 1, \"color\": \"b\"})\n",
        "plt.xlabel('word_count')\n",
        "plt.ylabel('frequency\\'s of word count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5eMv5R1JL8O"
      },
      "source": [
        "### We will explore some **SpaCy** functionalities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlIGItroLePj"
      },
      "source": [
        "# Load spacy - English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKVraXOfm2cx"
      },
      "source": [
        "### **The dataset consists of the following tags**-\n",
        "---\n",
        "- geo = Geographical Entity\n",
        "- org = Organization\n",
        "- per = Person\n",
        "- gpe = Geopolitical Entity\n",
        "- tim = Time indicator\n",
        "- art = Artifact\n",
        "- eve = Event\n",
        "- nat = Natural Phenomenon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziScly1XPAas"
      },
      "source": [
        "### **Named Entity Recognition (NER) with SpaCy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALdtgXixLs-_"
      },
      "source": [
        "for sentence in data_duplicate['title'].sample(5, random_state = 5):\n",
        "  print(\"Sentence is: \", sentence)\n",
        "  sentence_doc = nlp(sentence)\n",
        "  displacy.render(sentence_doc,style='ent',jupyter=True)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCVgIGqTN-6A"
      },
      "source": [
        "# now let's take a look at one whole piece of news to get a better context of NER extractions\n",
        "\n",
        "for sentence in data_duplicate['text'].sample(1, random_state = 5):\n",
        "  print(\"Sentence is: \", sentence)\n",
        "  sentence_doc = nlp(sentence)\n",
        "  displacy.render(sentence_doc,style='ent',jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS-dsYl2PZg6"
      },
      "source": [
        "# CELL EXECUTION TAKES 15 MINS\n",
        "%%time\n",
        "nlp = spacy.load('en',\n",
        "                 disable=['parser',\n",
        "                          'tagger',\n",
        "                          'textcat'])\n",
        "frames = []\n",
        "for i in tqdm(range(len(data))):\n",
        "    doc = data_duplicate.loc[i,'clean_text_data']\n",
        "    text_id = data_duplicate.loc[i,'index']\n",
        "    doc = nlp(doc)\n",
        "\n",
        "    ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents if len(e.text.strip(' -—')) > 0]\n",
        "    frame = pd.DataFrame(ents)\n",
        "    frame['id'] = text_id\n",
        "    frames.append(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2umoCKO0S_TK"
      },
      "source": [
        "ner = pd.concat(frames)\n",
        "ner.columns = ['Text','Start','Stop','NER_Type','id']\n",
        "ner.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_NZP0zzb73Z"
      },
      "source": [
        "# Saving the file for future use\n",
        "ner.to_csv('/content/drive/My Drive/Colab Notebooks/Project/Kaggle - Fake-and-Real-News-Dataset/ner.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQGLXXNTGCJC"
      },
      "source": [
        "# Not required to execute if you've already executed the previous 3 cells\n",
        "ner = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Project/Kaggle - Fake-and-Real-News-Dataset/ner.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO7N44e4dLXo"
      },
      "source": [
        "### **NER Types**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhS094F-bZn0"
      },
      "source": [
        "color_list = list('rgbkymc')  #red, green, blue, black, etc.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.xlabel(\"NER Types\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.title('Different NER Types', fontsize=20)\n",
        "ner.NER_Type.value_counts().plot(kind='bar', color = color_list)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzxfbx1LdOhF"
      },
      "source": [
        "### **TOP 20 PERSON**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNohge3SbgDY"
      },
      "source": [
        "person = ner[ner.NER_Type == 'PERSON']\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.xlabel(\"Top 20 people mentioned\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.title('TOP 20 PERSON', fontsize=20)\n",
        "person.Text.value_counts()[:20].plot(kind='bar')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCV5LmhvdSvR"
      },
      "source": [
        "### **TOP 20 Organization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbQA81KJbj2k"
      },
      "source": [
        "orgs = ner[ner.NER_Type == 'ORG']\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.xlabel(\"Top 20 Organizations mentioned\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.title('Top 20 Organization Types', fontsize=20)\n",
        "orgs.Text.value_counts()[:20].plot(kind='barh')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQEVfMxidZh_"
      },
      "source": [
        "### **TOP 20 Places**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ruw0haRbuB4"
      },
      "source": [
        "place = ner[ner.NER_Type == 'GPE']\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.xlabel(\"Top 20 Places mentioned\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.title('Top 20 Places', fontsize=20)\n",
        "place.Text.value_counts()[:20].plot(kind='barh')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__oHx6tVNgLF"
      },
      "source": [
        "### **Word Cloud**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWq_gERcJM3m"
      },
      "source": [
        "Next we like to see what are the most common words in real/fake news to discover some patterns. Word cloud is a popular way to visualize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvAu8Bod9TCo"
      },
      "source": [
        "def plot_wordcloud(target,width = 800, height = 400):\n",
        "    \"\"\"\n",
        "    Plot wordcloud of real/fake news\n",
        "\n",
        "    target: real/fake\n",
        "    width: the width of plotted figure\n",
        "    height: the height of plotted figure\n",
        "    \"\"\"\n",
        "    if target == 'real':\n",
        "        t = 1\n",
        "    elif target == 'fake':\n",
        "        t = 0\n",
        "    text = ''\n",
        "    for t in data_duplicate['clean_text_data'][data_duplicate['label'] == t]:\n",
        "        text = text + t\n",
        "    wordcloud = WordCloud(max_font_size=40, min_font_size=20, width=800, height = 400, random_state=0).generate(text)\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQE2XuEm9vvC"
      },
      "source": [
        "%%time\n",
        "plot_wordcloud('real',width = 800, height = 400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGnhbVnZ-gJO"
      },
      "source": [
        "%%time\n",
        "plot_wordcloud('fake',width = 800, height = 400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anspdXS5u8tR"
      },
      "source": [
        "### **N-Gram Exploration**\n",
        "\n",
        "To analyse the text column we will be extracting the N-Gram features.\n",
        "\n",
        "N-grams are used to describe the number of words used as observation points, e.g., **unigram** means singly-worded, **bigram** means 2-worded phrase, and **trigram** means 3-worded phrase. Here is a nice way to understand this\n",
        "\n",
        "<p align = \"center\"><img src=\"https://i.stack.imgur.com/8ARA1.png\"/></p>\n",
        "\n",
        "\n",
        "**Source**: https://stackoverflow.com/questions/18193253/what-exactly-is-an-n-gram\n",
        "\n",
        "In order to do this, we will use scikit-learn’s CountVectorizer function. The Scikit-Learn's CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.You can read more about the it [here](https://www.kaggle.com/parulpandey/getting-started-with-nlp-feature-vectors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRrDJCbP5f8j"
      },
      "source": [
        "#Bi-grams\n",
        "\n",
        "def plot_top_ngrams_barchart(text , n=2):\n",
        "\n",
        "  stop=set(stopwords.words('english'))\n",
        "\n",
        "  new= text.str.split()\n",
        "  new=new.values.tolist()\n",
        "  corpus=[word for i in new for word in i]\n",
        "\n",
        "  def _get_top_ngram(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx])\n",
        "                      for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:20]\n",
        "\n",
        "  top_n_bigrams=_get_top_ngram(text,n)[:20]\n",
        "  x,y=map(list,zip(*top_n_bigrams))\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.xlabel(\"Bi-gram Frequency\")\n",
        "  plt.ylabel(\"Top 20 bi-grams mentioned in News Title\")\n",
        "  sns.barplot(x=y,y=x)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LmSNNL9t43X"
      },
      "source": [
        "#### **Bi-Gram plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrDt_9TL7nl1"
      },
      "source": [
        "plot_top_ngrams_barchart(data_duplicate['clean_text_data'],2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc3sSfXuAqi"
      },
      "source": [
        "#### **Tri-Grams plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQIOeOfl2r3Y"
      },
      "source": [
        "plot_top_ngrams_barchart(data_duplicate['clean_text_data'],3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiokOwCNLx84"
      },
      "source": [
        "## **Count vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deLszaA-GWoD"
      },
      "source": [
        "%%time\n",
        "\n",
        "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
        "counts = count_vectorizer.fit_transform(data_duplicate['clean_text_data'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwQUOzBGFLy6"
      },
      "source": [
        "print(\"some feature names \", count_vectorizer.get_feature_names()[0:10])\n",
        "print('='*50)\n",
        "print(\"the type of count vectorizer \",type(counts))\n",
        "print(\"the shape of out text BOW (bag of words) vectorizer \",counts.get_shape())\n",
        "print(\"the number of unique words \", counts.get_shape()[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH2IaAT-F4r0"
      },
      "source": [
        "print(\"some feature names \", count_vectorizer.get_feature_names()[500:550])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQtp1DBFeUiq"
      },
      "source": [
        "## **Train Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmepUwIFb2D3"
      },
      "source": [
        "# Dividing the training set by using train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(data_duplicate['clean_text_data'],data_duplicate['label'], test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdYZo33VxJRq"
      },
      "source": [
        "print(\"X_train shape : {}\".format(X_train.shape))\n",
        "print(\"-\"*50)\n",
        "print(\"y_train shape : {}\".format(y_train.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nkI2zWaxUKO"
      },
      "source": [
        "print(\"X_val shape : {}\".format(X_val.shape))\n",
        "print(\"-\"*50)\n",
        "print(\"y_val shape : {}\".format(y_val.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olgL813Uecd4"
      },
      "source": [
        "## **BIDRECTIONAL LSTM MODEL**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wixxCXc5Vzbu"
      },
      "source": [
        "#@title Dataloader { display-mode: \"form\" }\n",
        "max_features = 10000 #@param {type:\"integer\"}\n",
        "embedding_vector = 300 #@param {type:\"slider\", min:0, max:300, step:1}\n",
        "EPOCHS = 5 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "BATCH_SIZE = 128 #@param [\"32\", \"64\", \"128\", \"256\"] {type:\"raw\"}\n",
        "max_len =  500#@param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgH5xfg6z7St"
      },
      "source": [
        "## **Keras - Tokenizer**\n",
        "---\n",
        "\n",
        "This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n",
        "\n",
        "\n",
        "#### **Arguments**\n",
        "\n",
        "\n",
        "**`num_words`**: the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
        "\n",
        "**`filters`**: a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character.\n",
        "\n",
        "**`lower`**: boolean. Whether to convert the texts to lowercase.\n",
        "\n",
        "**`split`**: str. Separator for word splitting.\n",
        "\n",
        "**`char_level`**: if True, every character will be treated as a token.\n",
        "\n",
        "**`oov_token`**: if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_uuyaIEz6FJ"
      },
      "source": [
        "%%time\n",
        "\n",
        "# OOV Token = 'Out of vocabulary token'\n",
        "# Here UNK Refers to 'Unknown token'\n",
        "\n",
        "tokenizer = Tokenizer(num_words = max_features+1, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ', oov_token='UNK')\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-pvisnG2vMJ"
      },
      "source": [
        "**`fit_on_texts`** : Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
        "\n",
        "**`texts_to_sequences`** : Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
        "\n",
        "**Reference** : [https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do](https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gbnBnFx03-e"
      },
      "source": [
        "There some bugs in tokenizer while using num_words as argument, here is the code to fix this issue\n",
        "\n",
        "**Reference** : [Using Tokenizer with num_words #8092](https://github.com/keras-team/keras/issues/8092#issuecomment-372833486)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UH03eDOot5p"
      },
      "source": [
        "## **Save tokenizer file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWHmfq0mo5_M"
      },
      "source": [
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoI_aPmMpdfk"
      },
      "source": [
        "# Loading tokenizer\n",
        "'''\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn3IXViyy0Q4"
      },
      "source": [
        "# tokenizer.word_index = {e:i for e,i in tqdm(tokenizer.word_index.items()) if i <= max_features} # <= because tokenizer is 1 indexed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6IABdJrz51J"
      },
      "source": [
        "# tokenizer.word_index[tokenizer.oov_token] = max_features + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hJrxhGb566k"
      },
      "source": [
        "import pprint\n",
        "for key,value in tokenizer.word_index.items():\n",
        "  pprint.pprint(\"{} : {}\".format(key,value))\n",
        "  if value==10:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWfLgFWo3HLq"
      },
      "source": [
        "## **Apply padding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1p_OniHqJ_q"
      },
      "source": [
        "**Why we use padding=pre ?**\n",
        "\n",
        "Answer :\n",
        "\n",
        "Commonly in RNN's, we take the final output or hidden state and use this to make a prediction (or do whatever task we are trying to do).\n",
        "\n",
        "If we send a bunch of 0's to the RNN before taking the final output (i.e. 'post' padding as you describe), then the hidden state of the network at the final word in the sentence would likely get 'flushed out' to some extent by all the zero inputs that come after this word.\n",
        "\n",
        "So intuitively, this might be why pre-padding is more popular/effective.\n",
        "\n",
        "**Reference** : [https://stackoverflow.com/questions/46298793/how-does-choosing-between-pre-and-post-zero-padding-of-sequences-impact-results](https://stackoverflow.com/questions/46298793/how-does-choosing-between-pre-and-post-zero-padding-of-sequences-impact-results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjRzDezQ7VXP"
      },
      "source": [
        "fig = px.histogram(data_duplicate,x = 'length',title='Histogram of Word_Count',nbins=100)\n",
        "fig.show(renderer=\"colab\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vym4I3Fw9aEP"
      },
      "source": [
        "```\n",
        "NOTE : Highest frequency of word count in the particular article belong in the range : 300 to 400.\n",
        "But we will kept max_len upto 500.  \n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpGxhN6h6-tb"
      },
      "source": [
        "# now applying padding to make them even shaped.\n",
        "X_train_padded = pad_sequences(tokenized_train, maxlen=max_len, padding = 'pre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roDvGQqZ7IDA"
      },
      "source": [
        "# 1.Total Number of document (sentences) 2. length of each document - 500\n",
        "\n",
        "X_train_padded.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7vAxoxt_Q4A"
      },
      "source": [
        "# FOR VALIDATION SET\n",
        "\n",
        "tokenized_test = tokenizer.texts_to_sequences(X_val)\n",
        "X_val_padded = pad_sequences(tokenized_test, maxlen=max_len, padding = 'pre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khu88z5nCTY-"
      },
      "source": [
        "## **Word Embedding**\n",
        "---\n",
        "\n",
        "Word embeddings provide a dense representation of words and their relative meanings.\n",
        "\n",
        "They are an improvement over sparse representations used in simpler bag of word model representations.\n",
        "\n",
        "```\n",
        "A word embedding is a class of approaches for representing words and documents using a dense vector representation.\n",
        "```\n",
        "<p align=\"center\"><img src=\"https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg\"/>\n",
        "\n",
        "It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n",
        "\n",
        "Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n",
        "\n",
        "The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
        "\n",
        "The position of a word in the learned vector space is referred to as its embedding.\n",
        "\n",
        "Two popular examples of methods of learning word embeddings from text include:\n",
        "```\n",
        "1. Word2Vec.\n",
        "2. GloVe.\n",
        "```\n",
        "In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. This can be a slower approach, but tailors the model to a specific training dataset.\n",
        "\n",
        "\n",
        "**Reference** :\n",
        "- [Towardsdatascience - Word Embedding [Word2Vec algorithm implementation]](https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8)\n",
        "\n",
        "- [Google Developers - Wordvec vector visualization](https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg)\n",
        "\n",
        "- [Tensorflow Projector - Word2vec Visualization](https://projector.tensorflow.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cuSTPDxChIG"
      },
      "source": [
        "## **Introduction to GloVe**\n",
        "---\n",
        "\n",
        "GloVe method is built on an important idea, You can derive semantic relationships between words from the co-occurrence matrix.\n",
        "\n",
        "```\n",
        "It learn the semantics not only from context words from the sentence but it learns the semantics from the global context.\n",
        "```\n",
        "\tGlobal Vectors for words representation (GloVe) is provided by Stanford.\n",
        "\tThey provided various models from 25, 50, 100, 200 to 300 dimensions based on 2, 6, 42, 840 billion tokens\n",
        "\tTeam used word-to-word co-occurrence to build this model. In other words, if two words co-occur many times,\n",
        "\tit means they have some linguistic or semantic similarity.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.ibb.co/c6nGM2L/Word2-Vec-vs-Glove.png\" alt=\"Word2-Vec-vs-Glove\" border=\"0\" width = \"80%\"></p>\n",
        "\n",
        "\n",
        "**Reference** : [Github - WomenWhoCode DataScience](https://github.com/WomenWhoCode/WWCodeDataScience/tree/master/Intro_to_NLP)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCPT3p8cpw_K"
      },
      "source": [
        "## **Downloading Glove Pretrained Embedding**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7g6elH-qKjU"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W2bHjrdrV_2"
      },
      "source": [
        "!unzip glove.840B.300d.zip -d glove_pretrained_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTV9f8vLrtPh"
      },
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "pathToGlove = 'glove_pretrained_embeddings/glove.840B.300d.txt'\n",
        "\n",
        "glove2word2vec(glove_input_file=pathToGlove, word2vec_output_file=\"gensim_glove_vectors.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1PkBBVhOuV-"
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)\n",
        "end = time.time()\n",
        "print(\"Total execution time in mins: {}\".format((end-start)//60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YIKmQPsgF9W"
      },
      "source": [
        "**Reference :** [Stackoverflow - load pretrained glove vectors in python](https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgEF5XgsbQ43"
      },
      "source": [
        "## **Save Glove Vector file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sYLlZa5XqQ0"
      },
      "source": [
        "!cp 'gensim_glove_vectors.txt' '/content/drive/My Drive/Colab Notebooks/Project/Kaggle - Fake-and-Real-News-Dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC3LSkRIMgN1"
      },
      "source": [
        "!gdown --id 1iVfRGxmycWe0kVeb7IoXViAZtfFFAkNw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg7rBpH4Sc0-"
      },
      "source": [
        "**Reference** : [Medium - How to directly download files from dropbox or google drive using wget in terminal or in google](https://kobkrit.com/how-to-directly-download-files-from-dropbox-or-google-drive-using-wget-in-terminal-or-in-google-573168195011)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqQXti28OwNL"
      },
      "source": [
        "# Length of vocabulary\n",
        "\n",
        "print(f\"There are {len(glove_model.vocab)} unique words in the vocab\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMl8juB8O519"
      },
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(glove_model.most_similar(positive=['car', 'minivan'], topn=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I9e0NkfPAX2"
      },
      "source": [
        "pprint.pprint(glove_model.most_similar(positive=['trump', 'clinton'], topn=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AdDQP-rPLN4"
      },
      "source": [
        "glove_model.most_similar('politics', topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYhMUu-1Eize"
      },
      "source": [
        "glove_model.most_similar('india', topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUfPaql9bp9w"
      },
      "source": [
        "## **Load Embedding Matrix Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDGkzbQZjYcV"
      },
      "source": [
        "print(len(tokenizer.word_index))\n",
        "print(max_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt1RP1BZbwg5"
      },
      "source": [
        "def loadEmbeddingMatrix(glove_model):\n",
        "  '''\n",
        "  Input :  Pretrained Embedding Model\n",
        "  Output : Embedding Matrix (based on embed_size)\n",
        "  embed_size = 300\n",
        "  '''\n",
        "\n",
        "  embed_size = 300\n",
        "  embeddings_index = dict()\n",
        "\n",
        "  for word in glove_model.vocab:\n",
        "    embeddings_index[word] = glove_model.word_vec(word)\n",
        "  print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  gc.collect()\n",
        "  # We get the mean and standard deviation of the embedding weights so that we could maintain the\n",
        "  # Same statistics for the rest of our own random generated weights.\n",
        "\n",
        "  all_embs = np.stack(list(embeddings_index.values()))\n",
        "  emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "\n",
        "  word_index_len = len(tokenizer.word_index)\n",
        "  nb_words = min(max_features, word_index_len)\n",
        "\n",
        "  # We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
        "  # the size will be Number of Words in Vocab X Embedding Size.\n",
        "  embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words+1, embed_size))\n",
        "  gc.collect()\n",
        "\n",
        "  # With the newly created embedding matrix, we'll fill it up with the words that we have in both\n",
        "  # Our own dictionary and loaded pretrained embedding.\n",
        "  embeddedCount = 0\n",
        "\n",
        "  for word, i in tokenizer.word_index.items():\n",
        "\n",
        "    if i > max_features:\n",
        "      continue\n",
        "\n",
        "    # then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    # and store inside the embedding matrix that we will train later on.\n",
        "\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "      embeddedCount+=1\n",
        "\n",
        "  print('total embedded:',embeddedCount,'common words')\n",
        "\n",
        "  # for word, i in tokenizer.word_index.items():\n",
        "  #   i-=1\n",
        "  #   # then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
        "  #   embedding_vector = embeddings_index.get(word)\n",
        "  #   # and store inside the embedding matrix that we will train later on.\n",
        "  #   if embedding_vector is not None:\n",
        "  #     embedding_matrix[i] = embedding_vector\n",
        "  #     embeddedCount+=1\n",
        "\n",
        "  del(embeddings_index)\n",
        "  gc.collect()\n",
        "\n",
        "  # finally, return the embedding matrix\n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTVUBMEKfaGZ"
      },
      "source": [
        "The function would return a new embedding matrix that has the loaded weights from the pretrained embeddings for the common words we have, and randomly initialized numbers that has the same mean and standard deviation for the rest of the weights in this matrix.\n",
        "\n",
        "\n",
        "**Reference** : [Kaggle : do pretrained embeddings give you the extra edge](https://www.kaggle.com/sbongo/do-pretrained-embeddings-give-you-the-extra-edge)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpqjOh7jdjDx"
      },
      "source": [
        "# Let's move on and load our first embeddings from Word2Vec.\n",
        "embedding_matrix = loadEmbeddingMatrix(glove_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqI1JbMsJcqR"
      },
      "source": [
        "# Try this one\n",
        "# glove_model['say'] - embedding_matrix[2]\n",
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI8yj0lzPBQa"
      },
      "source": [
        "## **Designing architecture of model**\n",
        "---\n",
        "<p align=\"center\"/>\n",
        "    <img src=\"https://miro.medium.com/max/700/1*HQeN5Q9FhN_XPbM4QuWIRg.jpeg\" width=\"60%\"/>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozi7CrC-rLE2"
      },
      "source": [
        "#### **Sequential API vs Functional API**\n",
        "---\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.stack.imgur.com/WCess.png\" width=\"40%\">\n",
        "</p>\n",
        "\n",
        "1. Keras Sequential Models\n",
        "\n",
        "  The Sequential model API is a way of creating deep learning models where an instance of the Sequential class is created and model layers are created and added to it.\n",
        "\n",
        "  The Sequential model API is great for developing deep learning models in most situations, but it also has some limitations.\n",
        "\n",
        "  For example, it is not straightforward to define models that may have multiple different input sources, produce multiple output destinations or models that re-use layers.\n",
        "\n",
        "\n",
        "2. Keras Functional Models\n",
        "\n",
        "  The Keras functional API provides a more flexible way for defining models.\n",
        "\n",
        "  It specifically allows you to define multiple input or output models as well as models that share layers. More than that, it allows you to define ad hoc acyclic network graphs.\n",
        "\n",
        "  Models are defined by creating instances of layers and connecting them directly to each other in pairs, then defining a Model that specifies the layers to act as the input and output to the model.\n",
        "\n",
        "Reference :\n",
        "\n",
        "[https://machinelearningmastery.com/keras-functional-api-deep-learning/](https://machinelearningmastery.com/keras-functional-api-deep-learning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V2tYywpHFx9"
      },
      "source": [
        "### **Embedding layer**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb4MNalPHJft"
      },
      "source": [
        "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
        "\n",
        "It is a flexible layer that can be used in a variety of ways, such as:\n",
        "\n",
        "* It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
        "\n",
        "* It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
        "\n",
        "* It can be used to load a pre-trained word embedding model, a type of transfer learning\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.ibb.co/yVSz7g3/Embedding-matrix.jpg\" alt=\"Embedding-matrix\" border=\"0\"></p>\n",
        "<br/>\n",
        "\n",
        "**Reference** :\n",
        "- [Machinelearningmastery - Use word embedding layers deep learning keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
        "\n",
        "- [Women Who Code Datascience - NLP Chapter - 3](https://drive.google.com/file/d/17349v5kb99qzpNvFuQkQrDRDbX0ZA6pN/view)\n",
        "\n",
        "- [Stack exchange : How does keras embedding layer work?](https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC4obW5fe7kT"
      },
      "source": [
        "## **Building Bidirectional LSTM Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqe57NBOzC0k"
      },
      "source": [
        "# Defining Neural Network\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer - 1st Layer\n",
        "# Non-trainable embeddidng layer\n",
        "model.add(Embedding(max_features+1, output_dim=embedding_vector, weights=[embedding_matrix], input_length=max_len, trainable=False,  name = '1st_layer'))\n",
        "\n",
        "\n",
        "# BILSTM layer - 128 Neuron (units) - 2nd layer\n",
        "# Dropout layer which indiscriminately \"disable\" some nodes so that the nodes\n",
        "# in the next layer is forced to handle the representation of the missing data\n",
        "# and the whole network could result in better generalization.\n",
        "\n",
        "# We set the dropout layer to drop out 25%(0.25) of the nodes.\n",
        "model.add(Bidirectional(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25,  name = '2nd_layer')))\n",
        "\n",
        "# BILSTM layer - 64 Neuron (units) - 3rd layer\n",
        "model.add(Bidirectional(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1,  name = '3rd_layer')))\n",
        "\n",
        "# Add Dense layer (Fully connected layer) - 4th layer\n",
        "# After a drop out layer, we connect the output of drop out layer to a densely connected layer\n",
        "# and the output passes through a RELU function. In short, this is what it does:\n",
        "\n",
        "#       Activation( (Input X Weights) + Bias)\n",
        "\n",
        "# all in 1 line, with the weights, bias and activation layer all set up for us!\n",
        "# We have defined the Dense layer to produce a output dimension of 120.\n",
        "model.add(Dense(units = 32 , activation = 'relu',  name = '4th_layer'))\n",
        "\n",
        "\n",
        "# Add Dense layer (Fully connected layer) - 5th & 6th layer (Output layer)\n",
        "\n",
        "# Finally, we feed the output into a Sigmoid layer.\n",
        "# The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0)\n",
        "# for each of the 2 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
        "model.add(Dense(1, activation='sigmoid', name = '5th_layer'))\n",
        "\n",
        "print(\"[INFO] Compiling model ...\")\n",
        "\n",
        "# compiling the model\n",
        "# All is left is to define the inputs, outputs and configure the learning process.\n",
        "# We have set our model to optimize our loss function using Adam optimizer,\n",
        "# define the loss function to be \"binary_crossentropy\" since we are tackling a binary classification.\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbANFpoGwWi_"
      },
      "source": [
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNtOkM9hKPmk"
      },
      "source": [
        "## **Callbacks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgJTRMbfUJ3k"
      },
      "source": [
        "### **`Early stoping`**\n",
        "A problem with training neural networks is in the choice of the number of training epochs to use.\n",
        "\n",
        "Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset.\n",
        "\n",
        "This callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process.\n",
        "\n",
        "The EarlyStopping callback is configured when instantiated via arguments.\n",
        "\n",
        "The “monitor” allows you to specify the performance measure to monitor in order to end training\n",
        "\n",
        "```python\n",
        " es = EarlyStopping(monitor='val_loss')\n",
        "```\n",
        "Based on the choice of performance measure, the “mode” argument will need to be specified as whether the objective of the chosen metric is to increase (maximize or ‘max‘) or to decrease (minimize or ‘min‘).\n",
        "\n",
        "For example, we would seek a minimum for validation loss and a minimum for validation mean squared error, whereas we would seek a maximum for validation accuracy.\n",
        "\n",
        "```python\n",
        "es = EarlyStopping(monitor='val_loss', mode='min')\n",
        "```\n",
        "By default, mode is set to ‘auto‘ and knows that you want to minimize loss or maximize accuracy.\n",
        "\n",
        "By setting `verbose` 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch. verbose=0 will show you nothing\n",
        "\n",
        "To discover the training epoch on which training was stopped, the “verbose” argument can be set to 1. Once stopped, the callback will print the epoch number.\n",
        "\n",
        "```python\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "```\n",
        "Often, the first sign of no further improvement may not be the best time to stop training. This is because the model may coast into a plateau of no improvement or even get slightly worse before getting much better.\n",
        "\n",
        "`patience` : Number of epochs with no improvement after which training will be stopped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNAaQk_Vxjsg"
      },
      "source": [
        "# Early Stop\n",
        "\n",
        "# To prevent over fitting we will stop the learning after 2 epochs and if val_loss value not decreased\n",
        "earlystop = EarlyStopping(monitor='val_loss',patience=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3YRyb8gauWb"
      },
      "source": [
        "### **`Learning Rate Reduction`**\n",
        "\n",
        "We will reduce the learning rate when then accuracy not increase for 3 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-sJu08wxsJ4"
      },
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                            patience=2,\n",
        "                                            verbose=1,\n",
        "                                            factor=0.5,\n",
        "                                            min_lr=0.00001\n",
        "                                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuGb2nYM5BJ6"
      },
      "source": [
        "### **`checkpoint`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyajbWJ67_Ni"
      },
      "source": [
        "```\n",
        "It is an approach where a snapshot of the state of the system is taken in case of system failure.\n",
        "\n",
        "If there is a problem, not all is lost.\n",
        "The checkpoint may be used directly, or used as the starting point for a new run, picking up where it left off.\n",
        "```\n",
        "When training deep learning models, the checkpoint is the weights of the model. These weights can be used to make predictions as is, or used as the basis for ongoing training.\n",
        "\n",
        "The Keras library provides a checkpointing capability by a callback API.\n",
        "\n",
        "The ModelCheckpoint callback class allows you to define where to checkpoint the model weights, how the file should named and under what circumstances to make a checkpoint of the model.\n",
        "\n",
        "The API allows you to specify which metric to monitor, such as loss or accuracy on the training or validation dataset. You can specify whether to look for an improvement in maximizing or minimizing the score. Finally, the filename that you use to store the weights can include variables like the epoch number or metric.\n",
        "\n",
        "The ModelCheckpoint can then be passed to the training process when calling the fit() function on the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoOPrIJeyms6"
      },
      "source": [
        "filepath=\"fake_news_bidirectional_glove.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, save_weights_only=True, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYA75RIMyy7_"
      },
      "source": [
        "tensorboard = TensorBoard(log_dir='./logs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zeLzDGhy2b2"
      },
      "source": [
        "callbacks = [earlystop,learning_rate_reduction,checkpoint,tensorboard]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoLGuNjfXfI6"
      },
      "source": [
        "X_train_padded.shape\n",
        "y_train.shape\n",
        "X_val_padded.shape\n",
        "y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZYlq7WZeyNV"
      },
      "source": [
        "### **Model fit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cagJMm8fy4ro"
      },
      "source": [
        "# initilize training config\n",
        "# batch size we have already consider\n",
        "\n",
        "# Run training\n",
        "print(\"[INFO] Training ...\")\n",
        "\n",
        "history = model.fit(X_train_padded,y_train,batch_size=BATCH_SIZE,epochs=EPOCHS,verbose=1,callbacks=callbacks, validation_data = (X_val_padded,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCM80s84EVGS"
      },
      "source": [
        "This model is actually trained on 9 epochs, because during 1st time due to overusage of RAM, session was crashed so i downloaded the checkpoints for 4 epoch.\n",
        "then again train the model with 5 epochs.\n",
        "\n",
        "**NOTE**: It takes approximately 5 hours for training for 9 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzqp7_sjGz-s"
      },
      "source": [
        "## **Saving Trained Model**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEFYiPDPzxVJ"
      },
      "source": [
        "# save model\n",
        "'''\n",
        "  Save Final Model\n",
        "  A final model is typically fit on all available data, such as the combination of all train and test dataset.\n",
        "\n",
        "  Note: saving and loading a Keras model requires that the h5py library is installed on your workstation.\n",
        "\n",
        "  The h5py package is a Pythonic interface to the HDF5 binary data format.\n",
        "  It lets you store huge amounts of numerical data, and easily manipulate that data from NumPy.\n",
        "'''\n",
        "\n",
        "model.save('Fake_News_Glove_Model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZdEE1YwaUG_"
      },
      "source": [
        "## **Load trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrFYasJoDEQk"
      },
      "source": [
        "# load model [NOT REQUIRED TO EXECUTE - IF TRAINED ON EPOCH]\n",
        "\n",
        "model = tf.keras.models.load_model('Fake_News_LSTM_Model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwsEQu-y4pTU"
      },
      "source": [
        "##**Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rgy8dqD36x"
      },
      "source": [
        "print('\\nhistory dict:', history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H1E3c_5D4fG"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# retrieve a list of list results on training and test data sets for each training epoch\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "lr = history.history['lr']\n",
        "\n",
        "epochs = range(len(acc))        ## get number of epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntPCeOA24tXp"
      },
      "source": [
        "### **Plot training and validation accuracy per epoch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R2TL5f-D7p2"
      },
      "source": [
        "'''\n",
        "  weird shape because we trained on less number of epoch\n",
        "'''\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(epochs, acc,label='train_accuracy')\n",
        "plt.plot(epochs, val_acc,label='validation_accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.title('Plot training and validation accuracy per epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnyemZHt4wKV"
      },
      "source": [
        "\n",
        "##**Plot training and validation loss per epoch**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXCu9etdEC7t"
      },
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(epochs, loss,label='train loss')\n",
        "plt.plot(epochs, val_loss,label='validation loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.title('Plot training and validation loss per epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vik7KBrX44b8"
      },
      "source": [
        "##**Show accuracy on the validation data**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1agTPpqEPWg"
      },
      "source": [
        "'''\n",
        "The evaluate_model() function below implements these behaviors,\n",
        "taking the validation data, and labels as arguments and returning a list of\n",
        "accuracy scores and training histories that can be later summarized.\n",
        "'''\n",
        "print(\"[INFO] Evaluating ... \")\n",
        "(loss,accuracy) = model.evaluate(X_val_padded,y_val,verbose=1)\n",
        "print(\"[INFO] accuracy : {:.2f}%\".format(accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH9JjGDkDwGh"
      },
      "source": [
        "## **Saving the weights**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_2L3hH6FuK5"
      },
      "source": [
        "## Save the weight\n",
        "\n",
        "# '''\n",
        "# # Here we are storing both model and weights\n",
        "# '''\n",
        "\n",
        "model.save_weights(\"weights/fake_news_bidirectional_glove_weights.temp.hd5\",overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUrhuKxpqB1J"
      },
      "source": [
        "### **Tensorboard in action**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPRSZ70QGV2W"
      },
      "source": [
        "# '''\n",
        "# If you are already in the directory where TensorFlow writes its logs, you should specify the port first:\n",
        "\n",
        "# tensorboard --port=6007 --logdir = logs\n",
        "# type localhost:6006 in chrome\n",
        "# '''\n",
        "\n",
        "# %load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir=logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2b93vl6cK1H"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "9mz7m5B5cKOD"
      },
      "source": [
        "fact_check = \"Donald Trump was born in Pakistan as Dawood Ibrahim Khan New Delhi: A video has gone viral showing a Pakistani anchor claiming that US President-elect Donald Trump was born in Pakistan and not in the United States of America.  The report further alleged that Trump's original name is Dawood Ibrahim Khan. In the video, the Neo News anchor elaborated on Trump's journey from North Waziristan to England and then finally to Queens, New York.  Neo news had cited tweets and a picture on social media to back its claim. The video was broadcast last month but went viral after Trump\\u2019s election victory on November 8.\" #@param {type:\"string\"}\n",
        "# fact_check = 'presidential candidate Donald Trump was born in Pakistan and not in America'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQsAVCfhHl8j"
      },
      "source": [
        "process_fact_check = process_text(fact_check)\n",
        "pprint.pprint(process_fact_check)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94rDM5g9HqNX"
      },
      "source": [
        "fact_check_sequence = tokenizer.texts_to_sequences([process_fact_check])\n",
        "print(fact_check_sequence)\n",
        "\n",
        "# now applying padding to make them even shaped.\n",
        "fact_check_padding = pad_sequences(sequences = fact_check_sequence, maxlen = max_len, padding = 'pre')\n",
        "print(fact_check_padding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9Y3wHb9Hw5g"
      },
      "source": [
        "fact_check_padding.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTwaCEMWfrqC"
      },
      "source": [
        "## **Result of test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulrTTUAwIx6n"
      },
      "source": [
        "button = widgets.Button(description=\"See the text 👀\",layout=Layout(width='50%', height='80px'))\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "  # Display the message within the output widget.\n",
        "  with output:\n",
        "    print(BOLD + YELLOW)\n",
        "    print(\"Input text :  \", end=\"\\t\")\n",
        "    print(BOLD + WHITE)\n",
        "    print(fact_check)\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "display(button, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvUbNUcJI_mh"
      },
      "source": [
        "button = widgets.Button(description=\"Fack Checking Result 👀\",layout=Layout(width='50%', height='80px'))\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "  # Display the message within the output widget.\n",
        "  with output:\n",
        "    result = model.predict_classes(fact_check_padding)\n",
        "    print(BOLD + BLUE)\n",
        "    print(\"Fake : 0 and Real : 1\")\n",
        "\n",
        "    print(BOLD + GREEN)\n",
        "    print(\"Input text : \", fact_check)\n",
        "\n",
        "    print(BOLD + WHITE)\n",
        "    print('-'*100)\n",
        "\n",
        "    if result[0][0] == 1:\n",
        "      print(BOLD + LIGHTRED)\n",
        "      print(\"Input text is REAL\")\n",
        "    else:\n",
        "      print(BOLD + YELLOW)\n",
        "      print(\"Input text is FAKE\")\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "display(button, output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SNhgw__8iU7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}